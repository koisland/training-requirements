{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport math\nimport plotly\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom yellowbrick.target.feature_correlation import feature_correlation\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import GridSearchCV\n\ngend_sub_df = pd.read_csv(\"/kaggle/input/titanic/gender_submission.csv\")\ntrain_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\ntest_passenger_id = test_df.PassengerId","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-02-22T01:35:04.979604Z","iopub.execute_input":"2022-02-22T01:35:04.979978Z","iopub.status.idle":"2022-02-22T01:35:05.009422Z","shell.execute_reply.started":"2022-02-22T01:35:04.979939Z","shell.execute_reply":"2022-02-22T01:35:05.008337Z"},"trusted":true},"execution_count":171,"outputs":[]},{"cell_type":"code","source":"# Preprocessing & Setup\n# https://www.kaggle.com/alexisbcook/titanic-tutorial\n\n# PassengerId, Name, Ticket not useful features in prediction of survival.\nremoved_features = [\"PassengerId\", \"Name\", \"Ticket\"]\n\n# Fare, Cabin, and Embarked could be useful so check for missing values. \n# If greater than 20% missing, remove col. Cabin is removed.\nna_perc = {col: pd.isna(train_df[col]).sum() / len(train_df[col]) for col in train_df.columns}\nremoved_features.append(*list(filter(lambda x: na_perc[x] > 0.2, na_perc)))\n\n# Remove features that fail to meet 80% threshold of usable values.\ntrain_df = train_df.loc[:, np.setdiff1d(train_df.columns,removed_features)]\ntest_df = test_df.loc[:, np.setdiff1d(test_df.columns,removed_features)]\n\nnumer_features = [\"Age\", \"Fare\"]\ncateg_features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\"]\n\n# Fill nas with mode if categorical and median if measurement\nfor col in numer_features:\n    test_df[col] = test_df[col].fillna(test_df[col].median())\n    train_df[col] = train_df[col].fillna(train_df[col].median())\n\nfor col in categ_features:\n    test_df[col] = test_df[col].fillna(test_df[col].mode())\n    train_df[col] = train_df[col].fillna(train_df[col].mode())\n    \n# Pclass, Sex, SibSP, and Parch are categorical variables so need to replace with dummy vars.\nX_train = pd.get_dummies(train_df[categ_features])\nX_test = pd.get_dummies(test_df[categ_features])\n\n# join categ_df with numerical_df\nX_train = X_train.join(train_df.loc[:, numer_features])\nX_test = X_test.join(test_df.loc[:, numer_features])\n\ny_train = train_df.loc[:, \"Survived\"]","metadata":{"execution":{"iopub.status.busy":"2022-02-22T01:35:05.011258Z","iopub.execute_input":"2022-02-22T01:35:05.011532Z","iopub.status.idle":"2022-02-22T01:35:05.059397Z","shell.execute_reply.started":"2022-02-22T01:35:05.011501Z","shell.execute_reply":"2022-02-22T01:35:05.058669Z"},"trusted":true},"execution_count":172,"outputs":[]},{"cell_type":"code","source":"# Feature Correlation\n# https://towardsdatascience.com/why-feature-correlation-matters-a-lot-847e8ba439c4\n# https://www.scikit-yb.org/en/latest/api/target/feature_correlation.html\n\n# Correlation of features with survival\nvisualizer = feature_correlation(X_train, y_train, labels=X_train.columns)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T01:35:05.061426Z","iopub.execute_input":"2022-02-22T01:35:05.061745Z","iopub.status.idle":"2022-02-22T01:35:05.300356Z","shell.execute_reply.started":"2022-02-22T01:35:05.061704Z","shell.execute_reply":"2022-02-22T01:35:05.299439Z"},"trusted":true},"execution_count":173,"outputs":[]},{"cell_type":"code","source":"# Create correlation matrix.\ncorr_mtx = X_train.corr()\nsns.heatmap(corr_mtx, annot=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T01:35:05.301862Z","iopub.execute_input":"2022-02-22T01:35:05.302459Z","iopub.status.idle":"2022-02-22T01:35:06.181981Z","shell.execute_reply.started":"2022-02-22T01:35:05.302414Z","shell.execute_reply":"2022-02-22T01:35:06.181340Z"},"trusted":true},"execution_count":174,"outputs":[]},{"cell_type":"code","source":"\n# https://towardsdatascience.com/pca-using-python-scikit-learn-e653f8989e60\n# Can only use measurement variables: Age and fare.\npca_df = train_df.loc[:, [\"Age\", \"Fare\"]]\n\n# Feature scaling: standardize values so that mean = 0 and variance = 1 (i.e. properties of a standard normal distribution)\npca_df_standardized = StandardScaler().fit_transform(pca_df)\n\n# Fit and transform values to two principal components\npca = PCA(n_components = 2)\npc = pca.fit_transform(pca_df_standardized)\n\npca_colnames = [f\"PC1 ({round(pca.explained_variance_ratio_[0], 3) * 100}%)\", f\"PC2 ({round(pca.explained_variance_ratio_[1], 3)* 100}%)\"]\ndf_pc = pd.DataFrame(pc, columns = pca_colnames)\ndf_pc[\"Survived\"] = train_df[[\"Survived\"]]\ndf_pc","metadata":{"execution":{"iopub.status.busy":"2022-02-22T01:35:06.184218Z","iopub.execute_input":"2022-02-22T01:35:06.184702Z","iopub.status.idle":"2022-02-22T01:35:06.210447Z","shell.execute_reply.started":"2022-02-22T01:35:06.184663Z","shell.execute_reply":"2022-02-22T01:35:06.209601Z"},"trusted":true},"execution_count":175,"outputs":[]},{"cell_type":"code","source":"sns.lmplot(x=df_pc.columns[0], y=df_pc.columns[1],\n  data=df_pc, \n  fit_reg=False, \n  hue='Survived', # color by if survived\n  legend=True,\n  scatter_kws={\"s\": 80}) ","metadata":{"execution":{"iopub.status.busy":"2022-02-22T01:35:06.211977Z","iopub.execute_input":"2022-02-22T01:35:06.212289Z","iopub.status.idle":"2022-02-22T01:35:06.689085Z","shell.execute_reply.started":"2022-02-22T01:35:06.212248Z","shell.execute_reply":"2022-02-22T01:35:06.688119Z"},"trusted":true},"execution_count":176,"outputs":[]},{"cell_type":"code","source":"# Random Forest Classifier\n# https://www.datacamp.com/community/tutorials/random-forests-classifier-python\nn_features = int(math.sqrt(len(X_train.columns)))\nclf = RandomForestClassifier(n_estimators = 50, random_state = 42, max_features=n_features)\nclf.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T01:35:06.690405Z","iopub.execute_input":"2022-02-22T01:35:06.691188Z","iopub.status.idle":"2022-02-22T01:35:06.823967Z","shell.execute_reply.started":"2022-02-22T01:35:06.691150Z","shell.execute_reply":"2022-02-22T01:35:06.822911Z"},"trusted":true},"execution_count":177,"outputs":[]},{"cell_type":"code","source":"# Feature Importance based on Mean Decrease of Impurity\n# https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n# Low cardinality features so should be okay?\nfeature_importance = pd.Series(clf.feature_importances_, X_train.columns).sort_values(ascending=False)\nsns.barplot(x=feature_importance, y = feature_importance.index)\nfeature_importance","metadata":{"execution":{"iopub.status.busy":"2022-02-22T01:35:06.825516Z","iopub.execute_input":"2022-02-22T01:35:06.826278Z","iopub.status.idle":"2022-02-22T01:35:07.100675Z","shell.execute_reply.started":"2022-02-22T01:35:06.826221Z","shell.execute_reply":"2022-02-22T01:35:07.099773Z"},"trusted":true},"execution_count":178,"outputs":[]},{"cell_type":"code","source":"# Remove Embarked columns due to low importance.\nlow_importance_cols = feature_importance[feature_importance < 0.05].index\nX_train = X_train.drop(columns = low_importance_cols)\nX_test = X_test.drop(columns = low_importance_cols)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T01:35:07.101868Z","iopub.execute_input":"2022-02-22T01:35:07.102516Z","iopub.status.idle":"2022-02-22T01:35:07.109705Z","shell.execute_reply.started":"2022-02-22T01:35:07.102480Z","shell.execute_reply":"2022-02-22T01:35:07.108928Z"},"trusted":true},"execution_count":179,"outputs":[]},{"cell_type":"code","source":"# Fit updated data to rf classifier and predict.\nn_features = int(math.sqrt(len(X_test.columns)))\nclf = RandomForestClassifier(n_estimators = 50, random_state = 42, max_features=n_features)\nclf.fit(X_train, y_train)\n\ny_pred = clf.predict(X_test)\nres = pd.DataFrame({\"PassengerId\": test_passenger_id, \"Survived\": y_pred})\nres.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T01:35:07.110898Z","iopub.execute_input":"2022-02-22T01:35:07.111349Z","iopub.status.idle":"2022-02-22T01:35:07.259675Z","shell.execute_reply.started":"2022-02-22T01:35:07.111310Z","shell.execute_reply":"2022-02-22T01:35:07.258880Z"},"trusted":true},"execution_count":180,"outputs":[]},{"cell_type":"code","source":"# param_grid = {\n#                  'n_estimators': [5, 10, 50, 500, 1000],\n#                  'max_features': [3, 5]\n#              }\n\n# grid_clf = GridSearchCV(clf, param_grid, cv=10)\n# grid_clf.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-02-22T01:35:07.261204Z","iopub.execute_input":"2022-02-22T01:35:07.261721Z","iopub.status.idle":"2022-02-22T01:35:07.265173Z","shell.execute_reply.started":"2022-02-22T01:35:07.261685Z","shell.execute_reply":"2022-02-22T01:35:07.264444Z"},"trusted":true},"execution_count":181,"outputs":[]},{"cell_type":"code","source":"# grid_clf.best_params_","metadata":{"execution":{"iopub.status.busy":"2022-02-22T01:35:07.266172Z","iopub.execute_input":"2022-02-22T01:35:07.267047Z","iopub.status.idle":"2022-02-22T01:35:07.278023Z","shell.execute_reply.started":"2022-02-22T01:35:07.266976Z","shell.execute_reply":"2022-02-22T01:35:07.277302Z"},"trusted":true},"execution_count":182,"outputs":[]}]}